% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/gbm_baseline.R
\name{gbm_baseline}
\alias{gbm_baseline}
\title{Gradient boosting machine baseline model function.}
\usage{
gbm_baseline(train_path = NULL, pred_path = NULL, days_off_path = NULL,
  train_Data = NULL, pred_Data = NULL, k_folds = 5,
  variables = c("Temp", "tow"), ncores = parallel::detectCores(logical = F),
  cv_blocks = "weeks", iter = seq(from = 50, to = 300, by = 25),
  depth = c(3:7), lr = c(0.05, 0.1), subsample = c(0.5),
  verbose = FALSE)
}
\arguments{
\item{train_path}{The path of the file from which the training data are to be read.}

\item{pred_path}{The path of the file from which the prediction data are to be read.}

\item{days_off_path}{The path of the file from which the date data of days off (e.g., holidays) are to be read.}

\item{train_Data}{A dataframe, of the training period, where the columns correspond to the time steps (time), the energy load (eload) and to the Temperature (Temp).}

\item{pred_Data}{A dataframe, of the prediction period, where the columns correspond to the time steps (time), the energy load (eload) and to the Temperature (Temp).}

\item{k_folds}{An integer that corresponds to the number of CV folds.}

\item{variables}{A vector that contains the names of the variables that will be considered by the function
as input variables.}

\item{ncores}{Number of threads used for the parallelization of the cross validation.}

\item{cv_blocks}{Type of blocks for the cross validation; Default is "none", which correspond
to the standard k-fold cross validation technique.}

\item{iter}{A vector with combination of the number of iterations.}

\item{depth}{A vector with combination of the maximum depths.}

\item{lr}{A vector with combination of the learning rates.}

\item{subsample}{A vector with combination of subsamples.}
}
\value{
a gbm_baseline object, which alist with the following components:
\describe{
  \item{gbm_model}{an object that has been created by the function xgboost,
   and which correspond to the optimal gbm model.}
  \item{train}{a dataframe that correspond to the training data after the
  cleaning and filtering function were applied.}
  \item{fitting}{the fitted values.}
  \item{goodness_of_fit}{a dataframe that contains the goodness of fitting metrics.}
  \item{gbm_cv_results}{a dataframe the training accuracy metrics (R2,
  RMSE and CVRMSE) and values of the tuning hype-parameters.}
  \item{tuned_parameters}{a list of the best hyper-parameters}
  \item{pred}{a dataframe that correspond to the prediction data after the
  cleaning and filtering function were applied.}
  \item{prediction}{the predicted values.}
}
}
\description{
\code{gbm_baseline} This function builds a baseline model using gradient boosting machine algorithm.
}
