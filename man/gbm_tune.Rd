% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/gbm_baseline.R
\name{gbm_tune}
\alias{gbm_tune}
\title{Gradient boosting machine tuning function.}
\usage{
gbm_tune(Data, k_folds, variables = c("Temp", "tow"), ncores,
  cv_blocks = "none", iter, depth, lr, subsample)
}
\arguments{
\item{Data}{A dataframe.}

\item{k_folds}{An integer that corresponds to the number of CV folds.}

\item{variables}{A vector that contains the names of the variables that will be considered by the function
as input variables.}

\item{ncores}{Number of threads used for the parallelization of the cross validation}

\item{cv_blocks}{type of blocks for the cross validation; Default is "none", which corresponds
to the standard cross validation technique}

\item{iter}{The search grid combination of the number of iterations}

\item{depth}{The search grid combination of the maximum depths}

\item{lr}{The search grid combination of the learning rates}
}
\value{
a list with the two following components:
\describe{
  \item{grid_results}{a dataframe the training accuracy metrics (R2,
  RMSE and CVRMSE) and values of the tuning hype-parameters }
  \item{tuned_parameters}{a list of the best hyper-parameters}
}
}
\description{
\code{gbm_tune} splits the data into k folds by randomly selecting blocks of data,
where each block correspond to a calendar day.
}
